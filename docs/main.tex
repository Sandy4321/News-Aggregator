\documentclass[a4paper, 14pt]{extarticle}
\usepackage{geometry}

\usepackage{cmap} % Улучшенный поиск русских слов в полученном pdf-файле
\usepackage{mathtext} % русские буквы в формулах
\defaulthyphenchar=127 % Если стоит до fontenc, то переносы не впишутся в выделяемый текст при 
%копировании его в буфер обмена
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{pscyr}  
\renewcommand{\rmdefault}{ftm} % ftm - (TimesNewRoman), fac - Academy, fad - Advertisement, flz - 
%Lazurski, fcr - CourierNewPSM, others in pscyr.sty

\usepackage{amsthm,amsfonts,amsmath,amssymb,amscd} % Математические дополнения от AMS
\usepackage{mathtools} % Добавляет окружение multlined

\usepackage{longtable} % Длинные таблицы
\usepackage{multirow,makecell,array} % Улучшенное форматирование таблиц
\usepackage{booktabs} % Возможность оформления таблиц в классическом книжном стиле

\usepackage{soulutf8} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний
\usepackage{icomma} % Запятая в десятичных дробях

\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}

\usepackage{hyperref}

\usepackage{graphicx} % Подключаем пакет работы с графикой
\graphicspath{{../images/}{images/}} % Пути к изображениям

%%% Подписи %%%
\usepackage[singlelinecheck=off,center]{caption}
\usepackage{subcaption}

\usepackage[onehalfspacing]{setspace}

%%% Списки %%%
\usepackage{enumitem}

%%% Библиография %%%
\usepackage{cite} % Красивые ссылки на литературу

%%% Оглавление %%%
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{titlesec}

\usepackage{titlesec} % Растояние между заголовками и текстом
\usepackage{float}
\usepackage{listings} % Listings

\usepackage{lipsum}

\input{styles} % Файл со стилями
\begin{document}
\include{cover_page} % Титульник
\include{abstract} % Аннотация
\include{table_of_content} % Оглавление 
% Тут комменты по всему:

\section{Введение}
% Неможко публицистическое введение, чтобы хорошо читалось
% ибо кроме введения обычно ничего не читают
С каждым годом вычислительные мощности современных компьютеров и сервисов, предлагающие облачные вычисления, позволяют 
обрабатывать всё большие массивы данных. Благодаря этому происходит быстрое развитие алгоритмов анализа данных и машинного обучения. Результат работы этих алгоритмов можно увидеть в нашей повседневной жизни: 
сервисы прогноза погоды, которые предсказывают направление движения облаков, персонализированная реклама, подстраивающиеся под 
интересы пользователя, автомобили с автопилотом и т.д. --- в основе всех этих разработок лежат алгоритмы интеллектуального анализа данных и машинного обучения.

Один из важнейших вопросов, который стоит перед исследователями и разработчиками подобных систем: где же взять данные для обучения моделей, с помощью которых можно анализировать новую информацию?

Ответ можно найти в Интернете --- это текст. Петабайты текста, написанного на естественном языке за последние несколько 
десятилетий и практически всегда имеющего смысл, доступны любому исследователю. Книги, новостные статьи, блоги, посты в 
социальных сетях, содержащие пусть и не очень литературный язык --- всё это достаточно просто извлечь.

Именно поэтому обработка естественного языка (Natrual Language Processing, NLP) получило большое развитие. Существуют развивающиеся проекты, коммерческие и с открытым исходным кодом, которые опираются на NLP. Это, например, IBM 
Watson --- набор продуктов, предлагающий множество инструментов работы с текстом: от извлечения важной информации и 
классификации, до его генерации. %https://www.ibm.com/watson/
NLP используется в поисковике и переводчике компании Яндекс, использующей для анализа алгоритм MatrixNet. %https://yandex.com/company/technologies/matrixnet/
Существуют специально созданные для NLP свободнораспространяемые библиотеки для разработчиков: NLTK, Gensim, Apache 
OpenNLP, Stanford CoreNLP и многие другие.

Все стандартные и базовые способы обработки естественного языка в исследовательских работах и в примерах использования 
NLP-инструментов обычно применяются на английском языке, но подразумевается, что все методы применимы для любого 
языка, при условии некоторых изменений в нормализации текста.
% TODO: Перейти к конкретике работы плавнее и сделать меньше пересказа содержание -- это в аннотации
В данной работе решаются две задачи NLP: это классификация и агрегация текста. Для придания работе новизны и 
оригинальности задача решается для новостных статей, написанных на русском языке. Востребованность этих задач 
показывает полностью автоматизированный новостной агрегатор компании Яндекс -- сервис Яндекс.Новости, который имеет 
огромную аудиторию. % https://yandex.ru/blog/company/76641

Чтобы показать практическую значимость работы, реализован новостной агрегатор в виде web-сервиса, показывающий работу 
алгоритмов на постоянно обновляющихся актуальных новостях русскоязычных интернет-СМИ. Важно отметить, что точность 
реализованных решений в процессе данной работы высокая, но несоизмеримая с тем же сервисом Яндекс.Новости, команда 
которого работает над своими алгоритмами годами, и даже при этом сервис работает не всегда точно. %https://yandex.ru/blog/company/novosti-za-voskresene

% Какие то общие слова, почему мы делаем этот агрегатор, какие у него будут функции.
\section{Цель и задачи курсовой работы}
Целью курсовой работы является разработка веб-сервиса, который:
\begin{enumerate}
	\item в реальном времени получает статьи из русскоязычных новостных источников;
	\item классифицирует полученные статьи по общим темам;
	\item агрегирует по схожести содержания статьи из различных источников.
\end{enumerate}
Агрегация и классификация основана на исследуемых в работе алгоритмы.

Для достижения поставленной цели, должны быть выполнены следующие задачи:
\begin{enumerate}
	\item Изучить методы и модели автоматической обработки текста и естественного языка;
	\item Собрать корпус новостных статей для обучения моделей;
	\item Исследовать и реализовать различные подходы к классификации и агрегации текстовых документов;
	\item Разработать back-end и front-end инфраструктуру сервиса.
\end{enumerate}

\section{Сбор и подготовка данных}
\subsection{Получение данных из новостных источников}
Для получения робастных моделей машинного обучения, требуется достаточно большой корпус новостных статей,
содержащий нескольких сотен тысяч документов. Обычно существуют готовые коллекции документов, но из-за выбранных 
ограничений к документам, а именно: новости на литературном русском языке, содержащие тег -- слово, описывающие тему 
документа, найти такой корпус практически невозможно, поэтому необходимо собрать данные самостоятельно.

В качестве новостных источников были выбраны следующие популярные СМИ:
<<Газета.Ru>>, <<Lenta.ru>>, <<ТАСС>>, <<Новая Газета>>, <<ВЕДОМОСТИ>> и <<СПОРТ-ЭКСПРЕСС>>. Последнее было выбрано
по причине малого количества спортивных новостей от других источников.

При анализе сайтов СМИ стало понятно, что они имеют схожую структуру: для отображения ссылок на статьи,
используются страницы со списком новостей (<<Лента новостей>>), по которым можно итерироваться,
изменяя параметры запросов, например, дату последней новости на странице или количество показанных статей.
Для извлечения данных из источников реализован набор парсеров, которые опираются на описанную структуру.
% Парсеры звучит привычно, но хз нормально ли тут юзать

Стоит отметить, что во многих случах для получения чистого текста приходится ждать ответа от сервера и обрабатывать 
HTML содержание страниц, и это сильно замедляет работу, поэтому парсеры одновременно обрабатывают множество ссылок
на статьи в параллельных процессах, что значительно ускоряет работу. Впоследствии эти парсеры удалось также внедрить в 
web-сервер для получения недавних новостей.

При формировании корпуса, все новостные статьи сопровождались различными метаданными: название СМИ, ссылка на статью,
дата публикации, тег и заголовок. В результате было получено более 1,1 млн. новостей от 1999 до 2017 года, многие из 
которых имели неправильно проставленные темы или не имели тем вовсе. Причин тому может быть несколько, например,
ошибки редакторов или технические ограничения веб-сайтов новостных агентств. Например, большинство новостей на сайте <<
Новой Газеты>> помечены тегом <<политика>>, хотя речь в них идет часто совсем о другом. После детального анализа тегов 
стало ясно, что относить новости к рубрикам редакторы стали только после определённого времени, а все уже имеющие 
статьи на сайте отнесли в <<политику>>. Некорректные данные пришлось удалить.

В итоговую выборку, которая в дальнейшем использовалась для обучения и тестирования алгоритмов,
вошло 133 тыс. статей, помеченных 32 различными тегами. Распределение СМИ и тем на отобранных данных отражено на 
рисунке \ref{media_topic_distr}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{media_topi_distr.pdf}
	\caption{Распределение СМИ и тем в данных}
	\label{media_topic_distr}
\end{figure}

%\lipsum[3-4]
%Заменил препроцессинг на нормализацию
\subsection{Нормализация новостных статей}
Нормализация является одной из важнейших стадий обработки естественного языка. Вкратце, нормализация приводит текст в 
более информативный для моделей вид. В зависимости от языка процесс нормализации может отличаться. Для русского языка, 
например, достаточно удалить пунктуацию, стоп-слова и лемматизировать. Стоп-слова -- это слова, которые примерно 
одинаково распределены по всему корпусу языка, чаще всего ими являются местоимения, предлоги и союзы. Лемматизация -- 
приведение слова в начальную форму. Часто вместо лемматизации используется стемминг -- алгоритм, который убирает части 
слова, влияющие на его форму, например, окончание. Данные алгоритмы не только опираются на словари, но и на 
определённые правила, зависящие от языка, так как в корпусе могут встречаться слова в разных формах, которых нет в 
словаре, например, неологизмы, но образованы они по правилам языка.

Процесс обработки текста в данной работе состоит из нескольких последовательных этапов:
\begin{enumerate}
	\item Приведение текста в нижний регистр.
	\item Удаление чисел и символов пунктуации. Дефис сохраняется.
	\item Удаление стоп-слов. В данный набор входят наиболее часто употребляемые слова русского и английского языков,
	а также названия новостных агентств (<<лента>>, <<тасс>>, <<риа>> и т.д.), сохранение которых приводит к 
	переобучению моделей.
	\item Лемматизация каждого слова с помощью библиотеки MyStem\footnote{\url{https://tech.yandex.ru/mystem/}}.
\end{enumerate}
%Исходный код конвеера можно найти в приложении. %TODO: добавить ссылку

\section{Классификация новостных статей}
%\lipsum[3-4]
\subsection{Линейная модель на TF-IDF признаках}
%\lipsum[3-4]
\subsection{Градиентный бустинг деревьев на word2vec}
%\lipsum[3-4]
\section{Агрегация новостных статей}
\subsection{Кластеризация с помощью алгоритмов машинного обучения}
\subsection{Объединение в связные компоненты графа}
%\lipsum[3-4]
\section{Разработка веб-приложения}
Веб-приложение состоит из двух основных частей: back-end и front-end. Back-end -- это сам веб-сервер,
который осуществляет обработку запросов пользователей, получение и обработку данных.
Front-end -- это пользовательский интерфейс, визуализирующий полученные данные от back-end в понятный вид.
С помощью этого интерфейса пользователь способен ни только получать, но и передавать данные на back-end.
% \subsection{Back-end часть}
% \subsection{Front-end часть}

\section{Заключение}

\include{bibliography} % Список литературы

\setcounter{secnumdepth}{0}
\section{Приложение}

\end{document}