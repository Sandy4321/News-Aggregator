\documentclass[a4paper, 14pt]{extarticle}
\usepackage{geometry}

\usepackage{cmap} % Улучшенный поиск русских слов в полученном pdf-файле
\usepackage{mathtext} % русские буквы в формулах
\defaulthyphenchar=127 % Если стоит до fontenc, то переносы не впишутся в выделяемый текст при 
%копировании его в буфер обмена
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{pscyr}  
\renewcommand{\rmdefault}{ftm} % ftm - (TimesNewRoman), fac - Academy, fad - Advertisement, flz - 
%Lazurski, fcr - CourierNewPSM, others in pscyr.sty

\usepackage{amsthm,amsfonts,amsmath,amssymb,amscd} % Математические дополнения от AMS
\usepackage{mathtools} % Добавляет окружение multlined

\usepackage{longtable} % Длинные таблицы
\usepackage{multirow,makecell,array} % Улучшенное форматирование таблиц
\usepackage{booktabs} % Возможность оформления таблиц в классическом книжном стиле

\usepackage{soulutf8} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний
\usepackage{icomma} % Запятая в десятичных дробях

\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}

\usepackage{hyperref}

\usepackage{graphicx} % Подключаем пакет работы с графикой
\graphicspath{{../images/}{images/}} % Пути к изображениям

%%% Подписи %%%
\usepackage[singlelinecheck=off,center]{caption}
\usepackage{subcaption}

\usepackage[onehalfspacing]{setspace}

%%% Списки %%%
\usepackage{enumitem}

%%% Библиография %%%
\usepackage{cite} % Красивые ссылки на литературу

%%% Оглавление %%%
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{titlesec}

\usepackage{titlesec} % Растояние между заголовками и текстом
\usepackage{float}
\usepackage{listings} % Listings

\usepackage{lipsum}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\input{styles} % Файл со стилями
\begin{document}
\include{cover_page} % Титульник
\include{abstract} % Аннотация
\include{table_of_content} % Оглавление 
% Тут комменты по всему:

\section{Введение}
% Неможко публицистическое введение, чтобы хорошо читалось
% ибо кроме введения обычно ничего не читают
С каждым годом вычислительные мощности современных компьютеров и сервисов, 
предлагающие облачные вычисления, позволяют 
обрабатывать всё большие массивы данных. Благодаря этому происходит быстрое 
развитие алгоритмов анализа данных и машинного обучения. Результат работы 
этих алгоритмов можно увидеть в нашей повседневной жизни: 
сервисы прогноза погоды, которые предсказывают направление движения 
облаков, персонализированная реклама, подстраивающиеся под 
интересы пользователя, автомобили с автопилотом и т.д. --- в основе всех 
этих разработок лежат алгоритмы интеллектуального анализа данных и 
машинного обучения.

Одна из важнейших проблем, возникающая перед исследователями и 
разработчиками подобных систем, заключается в поиске данных для обучения моделей,
которые в будущем будут использоваться для анализа новой информации.

Решением данной проблемы являются тексты из сети Интернет.
Петабайты информации, записанной на естественном языке за последние несколько 
десятилетий, доступны любому исследователю. Книги, новостные статьи, блоги, посты в 
социальных сетях --- всё это является источником легкодоступных данных.

Именно поэтому обработка естественного языка (Natrual Language Processing, 
NLP) получила большое развитие. За последние несколько лет появилось множество 
специализированных библиотек и сервисов для анализа естественного языка. К таким сервисам, 
например, относится IBM Watson --- набор платных продуктов, предлагающий различные
инструменты работы с текстом: от извлечения важной информации и 
классификации, до его генерации. %https://www.ibm.com/watson/
Яркими примерами свободнораспространяемых библиотек являются: NLTK, Gensim, MyStem,
pymorphy и многие другие.

В данной работе решаются две задачи:
\begin{enumerate}
	\item классификация тем новостных статей;
	\item кластеризация новостных статей по семантической близости.
\end{enumerate} 

Для придания работе новизны и оригинальности задача решается для русскоязычных новостей.

Практическая значимость работы доказывается на примере реализации в виде веб-сервиса новостного агрегатора,
который в реальном времени, с помощью алгоритмов и подходов предложенных в данной работе,
обрабатывает публикации русскоязычных интернет-СМИ.

% Какие то общие слова, почему мы делаем этот агрегатор, какие у него будут функции.
\section{Цель и задачи курсовой работы}
Целью курсовой работы является разработка веб-сервиса, который:
\begin{enumerate}
	\item в реальном времени получает статьи из русскоязычных новостных источников;
	\item классифицирует полученные статьи по общим темам;
	\item кластеризует по схожести содержания статьи из различных источников.
\end{enumerate}
Агрегация и классификация основана на исследуемых в работе алгоритмы.

Для достижения поставленной цели, должны быть выполнены следующие задачи:
\begin{enumerate}
	\item изучить методы и модели автоматической обработки текста и естественного языка;
	\item собрать корпус новостных статей для обучения моделей;
	\item исследовать и реализовать различные подходы к классификации и кластеризации текстовых документов;
	\item разработать back-end и front-end инфраструктуру сервиса.
\end{enumerate}

\section{Сбор и подготовка данных}
\subsection{Получение данных из новостных источников}
Для получения робастных моделей машинного обучения, требуется достаточно большой корпус новостных статей,
содержащий несколько сотен тысяч документов. Обычно существуют готовые коллекции текстов, но из-за выбранных 
ограничений к документам, а именно: новости на русском языке вместе с тег --- словом, описывающим тему 
документа, найти такой корпус не удалось, поэтому было принято решение собрать данные самостоятельно.

В качестве новостных источников были выбраны следующие популярные СМИ:
<<Газета.Ru>>, <<Lenta.ru>>, <<ТАСС>>, <<Новая Газета>>, <<ВЕДОМОСТИ>>, <<РИА Новости>> и <<СПОРТ-ЭКСПРЕСС>>.
Последнее было выбрано по причине малого количества спортивных новостей от других источников.

При анализе сайтов СМИ стало понятно, что они имеют схожую структуру: для отображения ссылок на статьи,
используются страницы со списком новостей (<<Лента новостей>>), по которым можно итерироваться,
изменяя параметры запросов, например, дату последней новости на странице или количество показанных статей.
Для извлечения данных из источников реализован набор алгоритмов, которые опираются на описанную структуру.

Стоит отметить, что во многих случаях для получения чистого текста приходится ждать ответа от сервера и обрабатывать 
HTML содержание страниц, что сильно замедляет работу, поэтому алгоритмы получения новостей одновременно обрабатывают
в параллельных процессах множество веб-страниц, что значительно ускоряет работу. Данные алгоритмы также используются в основном
веб-сервисе для получения недавних новостей.

При формировании корпуса, все новостные статьи сопровождались различными метаданными: название СМИ, ссылка на статью,
дата публикации, тег и заголовок. В результате было получено более 1,1 млн. новостей с 1999 по 2017 год, многие из 
которых имели неправильно проставленные темы или не имели тем вовсе. Причин тому может быть несколько, например,
ошибки редакторов или технические ограничения веб-сайтов новостных агентств. Например, большинство новостей на сайте
<<Новой Газеты>> помечены тегом <<политика>>, что зачастую не совпадает с истинным содержанием статьи. После детального анализа тегов 
стало ясно, что относить новости к рубрикам редакторы стали только после определённого времени, а все уже имеющие 
статьи на сайте отнесли в <<политику>>. Некорректные данные пришлось удалить.

В итоговую выборку, которая в дальнейшем использовалась для обучения и тестирования алгоритмов,
вошло $133\,529$ статей, помеченных 32 различными темами. Распределение СМИ и тем на отобранных данных отражено на 
рисунке \ref{media_topic_distr}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{media_topi_distr.pdf}
	\caption{Распределение СМИ и тем в данных}
	\label{media_topic_distr}
\end{figure}

%\lipsum[3-4]
%Заменил препроцессинг на нормализацию
\subsection{Нормализация новостных статей}
Нормализация является одной из важнейших стадий обработки естественного языка. Не формально, нормализация приводит текст в 
более информативный для моделей вид. В зависимости от языка процесс нормализации может отличаться. Например, для русского языка
зачастую удаляют пунктуацию и стоп-слова, а также производят лемматизацию. Стоп-слова --- это слова, которые примерно 
одинаково распределены по всему корпусу языка, чаще всего ими являются местоимения, предлоги и союзы. Лемматизация --- 
приведение слова в начальную форму (лемму):
\begin{itemize}
	\item для существительных --- именительный падеж, единственное число;
	\item для прилагательных --- именительный падеж, единственное число, мужской род;
	\item для глаголов, причастий, деепричастий --- глагол в инфинитиве.
\end{itemize}

Часто вместо лемматизации используется стемминг --- алгоритм, который убирает части 
слова, влияющие на его форму, например, окончание. В результате применения данной процедуры однокоренные слова, как правило,
преобразуются к одинаковому виду. Данные алгоритмы не только опираются на словари, но и на 
определённые правила, зависящие от языка, так как в корпусе могут встречаться слова в разных формах, которых нет в 
словаре, например, неологизмы, но образованы они по правилам языка.

Процесс обработки текста в данной работе состоит из нескольких последовательных этапов:
\begin{enumerate}
	\item приведение текста в нижний регистр;
	\item удаление чисел и символов пунктуации, дефис сохраняется;
	\item удаление стоп-слов. В данный набор входят наиболее часто употребляемые слова русского и английского языков
	а также названия новостных агентств (<<лента>>, <<тасс>>, <<риа>> и т.д.), сохранение которых приводит к 
	переобучению моделей;
	% TODO: Сослаться на майтем в литературе
	\item лемматизация каждого слова с помощью библиотеки MyStem\cite{mystem}.
\end{enumerate}
%Исходный код конвеера можно найти в приложении. %TODO: добавить ссылку

\section{Классификация новостных статей}
За последние два десятка лет, в результате активных исследований в области машинного обучения,
было изобретено множество успешных алгоритмов классификации. Например, такие модели как support vector machines (SVM) \cite{weston99svms},
градиентный бустинг над решающими деревьями и нейронные сети \cite{DBLP:journals/corr/ConneauSBL16}, были успешно применены к задачам классификации текстов.
В данной работе для классификации новостных статей использовались две из перечисленных выше модели --- это SVM и градиентный бустинг
над решающими деревьями.

Анализ текста является важной частью машинного обучения, однако сырые данные, а именно последовательности символов переменной длины,
не могут быть переданы на вход алгоритму в явном виде т.к. большинство моделей ожидают численный вектор признаков фиксированной длинны.

Векторизация --- метод трансформации коллекции текстовых документов в числовые вектора признаков.
Существуют различные подходы к векторизации текста, в данной работе были применены два самых популярных: TF-IDF и word2vec.

\subsection{Линейная модель на TF-IDF признаках}
Одним из самых простых подходов к решению задачи классификации текстовых документов является обучение линейного классификатора на
TF-IDF признаках, посчитанных на корпусе документов.

TF-IDF \cite{doi:10.1108/eb026526} --- статистическая мера, используемая для оценки важности слова в контексте 
документа, являющегося частью коллекции документов или корпуса.
TF-IDF --- это произведение двух статистик: TF (term frequency) и IDF (inverse 
document frequency). На сегодняшний день, TF-IDF один из самых популярных способов взвешивания слов, входящих в корпус документов.
Например, 83\% рекомендательных систем цифровых библиотек используют TF-IDF \cite{Beel2016}.

Существует множество способов подсчёта TF-IDF, в данной работе использовался следующий:
$$
\text{tf}(t, d) = \cfrac{n_t}{\sum_{k} n_k},
$$
где $n_{t}$ есть число вхождений слова $t$ в документ, а $\sum_{k} n_k$ --- общее число слов в данном документе.
$$
\text{idf}(t, D) = \log{ \cfrac{|D|}{|\{ d_i \in D \mid t \in d_i \}|}},
$$
где $|D|$ --- число документов в корпусе, $|\{ d_i \in D \mid t \in d_i \}|$ — число документов из коллекции $D$, в которых встречается 
$t$ (когда $n_{t} \neq 0$).

Таким образом, мера TF-IDF является произведением двух сомножителей:
$$
\text{TF-IDF}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D).
$$

Признаковым описанием одного объекта $d \in D$ будет вектор
$$
\big(\text{TF-IDF}(t,d,D)\big)_{t\in V},
$$
где $V$ --- словарь всех слов, встречающихся в коллекции $D$.

Для преобразования новостных статей в числовые признаки, использовался класс \verb+TfidfVectorizer+ из библиотеки машинного обучения Scikit-learn 
\cite{scikit-learn}. В качестве параметров векторизации использовались следующие значения: \verb+min_df=3+ --- учитываются слова, встретившиеся 
суммарно во всех документах минимум 3 раза и  \verb+ngram_range=(1,2)+~--- учитываются как отдельные слова, так и би-граммы.

После векторизации новостных статей, была получена разрежённая матрица размера $133\,529$ строк на $1\,025\,919$ столбцов. По причине большого 
количества признаков ($\gg$  обучающих примеров), в качестве классификатора было решено использовать линейный SVM. Не смотря на то, что данный 
алгоритм был впервые описан более пятидесяти лет назад, сегодня он по прежнему показывает одни из самых высоких результатов в задачах классификации 
текста. Как было показано в \cite{wang12simple}, особенно высокое качество удаётся получить при использовании би-грамм.

В данной работе в качестве SVM использовался \verb+SGDClassifier+ из библиотеки Scikit-learn. Данный класс реализует различные линейные
классификаторы, параметры которых оптимизируются с помощью алгоритма стохастического градиентного спуска \cite{Bottou2010}.

Классификатор обучался со следующими параметрами: \verb+loss="hinge"+~--- функционал качества линейного SVM,
\verb+n_iter=70+~--- число итераций оптимизационного алгоритма, \verb+alpha=1e-5+~--- коэффициент регуляризации.

Обучение классификатора происходило на $70\%$ новостных статей, остальные $30\%$ использовались для валидации.
Процесс разделения выборки был стратифицированнымы \footnote{Стратифицированная выборка --- выборка, в которой генеральная совокупность разделена на 
частичные совокупности, которые сами по себе должны быть однородными, а между собой --- разнородными (Финансовый словарь).}, в качестве
стратов выступали темы новостей.

Качество классификации оценивалось с помощью метрик Accuracy и F1-меры с макро-усреднением по каждому классу.
На валидационном множестве линейному SVM удалось получить $\text{Accuracy} = 0,8792$ и $\text{F1 score} = 0,8860$.

Из матрицы ошибок, приведённой на рисунке \ref{svm_confusion}, можно увидеть, что модель часто ошибается в классификации новостей на тему
<<экономика>>, относя их к новостям про <<бизнес>> (в $20\%$ случаев). Такая же ситуация характерна и для классов <<компании>>~-- 
<<бизнес>> ($12\%$), <<силовые структуры>>~-- <<оружие>> ($12\%$),  <<социальные сети>>~-- <<интернет>> ($15\%$), <<театр>>~-- 
<<кино>> ($9\%$). Перечисленные ошибки являются, в основном, причиной семантической схожести классов, но зачастую одна и та же 
новость может содержать различные темы, поэтому, возможно, нужно было решать данную задачу методами multilabel классификации, т.е. 
когда один объект может принадлежать сразу
нескольким классам.

Напротив, среди всех тем выделяются несколько, которые классифицируются с высокой долей точности. Данные темы 
являются узкоспециализированными и почти не имеют пересечения с другими. В основном это виды спорта, такие как <<футбол>> (точность 
$0,99$), <<хоккей>> ($0,99$), <<формула-1>> ($1,00$), <<кибер-спорт>> ($0,98$) и т.д. Более детальные
значения метрик по каждому из классов можно найти на рисунке \ref{svm_classif_report}.

Веса признаков в линейной модели в случае, если признаки отмасштабированы, характеризуют степень их влияния на значение целевой 
переменной. В задаче классификации текстов, кроме того, признаки являются хорошо интерпретируемыми, поскольку каждый из них 
соответствует конкретному слову, поэтому из линейного SVM были извлечены слова с наибольшим весом для каждой из 32 тем (таблица \ref{table:svm_topwords}). 
%Для определения этих метрик, введём следующие обозначения:
%
%TP (true postive)~--- число истинно-положительных значений.
%
%TN (true negative)~--- число истинно-отрицательных значений.
%
%FP (false positive)~--- число ложно-положительных значений.
%
%FN (false negative)~--- число ложно-отрицательных значений.
%
%$\text{precision (точность)} = \cfrac{TP}{TP + FP}$ --- насколько можно доверять классификатору.
%
%$\text{recall (полнота)} = \cfrac{TP}{TP + FN}$ --- как много объектов класса 1 находит классификатор.
%
%Тогда accuracy --- доля корректно классифицированных новостных статей от общего числа статей:
%$$
%\text{accuracy} = \cfrac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}.
%$$
%
%F1-мера --- гармоническое среднее точности и полноты
%$$
%\text{F1} = \cfrac{2 \cdot (\text{precision} \cdot \text{recall})}{\text{precision} + \text{recall}}.
%$$

\subsection{Градиентный бустинг над решающими деревьями. Word2Vec и тематическое моделирование.}
Другим популярным подходом к векторизации текста является алгоритм Word2Vec.
Реализация алгоритма, опубликованная компанией Google \cite{DBLP:journals/corr/abs-1301-3781} в 2013 году,
основывается на однослойной нейронной сети, которая пытается <<выучить>> векторные представления слов (англ. word embeddings).
До этого также были предложены различные архитектуры рекуррентных нейронных сетей, которые могли <<выучивать>> векторные 
представления, но их проблема заключалась в том, что они требовали намного больше времени для обучения, в отличии от реализации 
Word2Vec от Google.

Алгоритм Word2Vec является подмножеством более широкого класса алгоритмов, которые называются Vector space models (VSMs) 
\cite{Salton:1975:VSM:361219.361220}. VSMs представляют слова в непрерывном векторном пространстве, где семантически похожие
слова отображаются в близкие точки. VSMs имеют долгую и богатую историю в NLP, но все представители этого семейства моделей 
опираются на дистрибутивную семантику, которая утверждает, что слова появляющиеся в одном и том же контексте имеют близкое 
семантическое значение. Все методы, основанные на данной гипотезе, можно разделить на два класса: статистические, или численные (от 
англ. count-based, пр. Latent Semantic  Analysis) и предиктивные модели (пр. neural probabilistic language models).

Первый класс алгоритмов вычисляет как часто слова возникают в контексте своих соседей в больших корпусах текстов. Затем происходит
отображение вычисленных статистик в вектора для каждого слова. В свою очередь, предиктивные модели напрямую пытаются предсказать 
слова по его контексту, <<выучивая>> вектора таким образом, чтобы снизить ошибку предсказаний.

В частности, Word2Vec является вычислительно-эффективной реализацией предиктивной модели, которая <<выучивает>> векторные 
представления слов из необработанных текстовых данных. Есть две вариации алгоритма: Continuous Bag-of-Words (CBOW) и Skip-Gram. 
Алгоритмически они  похожи, за  исключением того, что CBOW предсказывает слово (например <<раму>>) основываясь на контексте (<<мама 
мыла>>), в то время как Skip-Gram наоборот, старается предсказать контекст по исходному слову. На практике CBOW лучше работает на 
небольших корпусах текстов, однако если  корпус достаточно большой, как в данной работе, то рекомендуется использовать модель, 
основанную на Skip-Gram.

В данной работе, для обучения модели Word2Vec использовалась библиотека Gensim \cite{rehurek_lrec}, в которой имеется обёртка над 
Word2Vec от Google  для языка Python. Модель обучалась со следующими параметрами: \verb|min_count=3|~--- в 
обучении используются только те слова,  которые встречаются во всём корпусе не менее трёх раз, \verb|vec_size=300|~--- размерность 
векторов слов, \verb|window=5|~--- размер окна (контекста), \verb|sg=1|~--- использовать алгоритм Skip-Gram, \verb|iter=10|~--- 
количество итераций по коллекции документов.

Результатом обучения Word2Vec является словарь, который отображает слова в вектора. Для представления новостной статьи в виде 
вектора признаков усредним все векторы слов в данной статье с idf весами. В результате получим матрицу, в которой $133\,529$ строк 
и $300$ столбцов (признаков), что значительно меньше, чем в случае TF-IDF. Благодаря тому, что удалось снизить размерность с 
$1\,025\,919$ признаков до $300$, становится возможным эффективное использование таких мощных алгоритмов как градиентный бустинг 
над решающими деревьями.

Градиентный бустинг над решающими деревьями --- один из самых универсальных и сильных методов машинного обучения, известных
на сегодняшний день. В последнее время этот алгоритм и, особенно его реализация в библиотеке XGBoost 
\cite{DBLP:journals/corr/ChenG16}, пользуются большой популярностью у участников соревнований в области анализа данных на платформе 
Kaggle.

В данной работе использовалась реализация градиентного бустинга из библиотеки LightGBM, которая является open-source проектом компании
Microsoft. Алгоритм построения деревьев в LightGBM немного отличается от того, как это реализовано в XGBoost. Несмотря на это интерфейс библиотек
и параметры алгоритмов зачастую полностью совпадают. Однако на практике LightGBM превосходит в скорости работы XGBoost, именно по этой причине
эта библиотека использовалась для решения задачи классификации.

Модель градиентного бустинга над деревьями обучалась на векторах, полученных с помощью Word2Vec, со следующими параметрами:
\begin{itemize}
	\item \verb|colsample_bytree=0.9| --- доля от общего числа признаков, используемая для построения дерева на каждой итерации;
	\item \verb|learning_rate=0.1| --- шаг градиентного спуска, темп обучения;
	\item \verb|max_depth=7| --- ограничение на максимальную глубину дерева;
	\item \verb|num_leaves=255| --- ограничение на максимальное число листьев в дереве;
	\item \verb|objective="multiclass"| --- функционал качества;
	\item \verb|metric="multi_error"| --- метрика, по которой оценивается качество и происходит ранняя остановка;
	\item \verb|subsample=0.8| --- доля от общего числа объектов, используемых для построения дерева на каждой итерации.
\end{itemize}

Итоговое качество алгоритма: $\text{Accuracy} = 0,8427$, $\text{F1 score} = 0,8486$.

Как видно из результатов, качество градиентного бустинга оказалось ниже линейного SVM, что, возможно, является причиной усреднения слов в документе,
во время которого теряется достаточно много информации. Одним из решений данной проблемы является генерация новых признаков. Для этого 
в работе было применено тематическое моделирование.

Тематическое моделирование --- активно развивающаяся ветвь статистического анализа текстов \cite{Blei:2012:PTM:2133806.2133826}. Тематические
модели стараются раскрыть скрытую тематическую структуру коллекции документов и найти сжатое представление каждого документа в виде
тем, которые в нём встречаются. Тематическое моделирование применяется в различных задачах, например, таких как информационный поиск,
классификация, кластеризация, сжатие текста без потери информации (summarization) и т.д. Различные способы и идеи применения тематических моделей
описаны в обзоре \cite{Daud2010}.

Статистически, модель вероятностного тематического моделирования определяет каждую тему как многомерное распределение над словами
$p\left(w \,\middle|\, t\right)$, где $w$~--- слово, $t$~--- тема, и затем описывает каждый документ в виде многомерного распределения тем $p\left(t \,\middle|\, d\right)$, где $d$~--- документ. Вероятностная тематическая модель выявляет скрытые темы по наблюдаемым распределениям слов в документах:
$$
p\left(w \,\middle|\, d\right) = \sum_{t \in T} p\left(w \,\middle|\, d, t\right) p\left(t \,\middle|\, d\right) = \sum_{t \in T} p\left(w \,\middle|\, t\right) p\left(t \,\middle|\, d\right).
$$

В данной работе для тематического моделирования использовалась библиотека bigARTM \cite{bigartm} --- открытая библиотека для тематического 
моделирования текстовых коллекций, реализующая алгоритм аддитивной регуляризации тематических моделей (ARTM). Модель обучалась на коллекции
новостных статей со следующими параметрами:
\begin{itemize}
	\item \verb|num_topics=150| --- количество тем, которые пытается выделить модель;
	\item \verb|tau=-0.2| --- параметр \verb|SmoothSparsePhiRegularizer|;
	\item \verb|tau=3e5| --- параметр \verb|DecorrelatorPhi|;
	\item \verb|tau=-0.001| --- параметр \verb|SmoothSparseThetaRegularizer|;
	\item \verb|num_collection_passes=50| --- число итераций оптимизационного алгоритма;
\end{itemize}

С помощью обученной модели были получены распределения тем для каждой новостной статьи.
Примеры распределений представлены в таблице \ref{bigartm_topics}. После чего, полученные векторы распределений
были сканкатенированы с векторами из Word2Vec, что в итоге увеличило число признаков с 300 до 450.

\begin{table}[h!]
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{llllllll}
			\toprule
			topic\_0   &        ученый &  исследование &         журнал &         вывод &     анализ &    исследователь &      изучать \\
			topic\_10  &        сделка &          доля &   принадлежать &         актив &    покупка &        продавать &  приобретать \\
			topic\_103 &  пользователь &        сервис &         google &      facebook &     яндекс &            поиск &       запрос \\
			topic\_105 &        сериал &         актер &        зритель &         герой &        шоу &           сердце &       эпизод \\
			topic\_16  &         нефть &        добыча &  месторождение &      нефтяной &      запас &          баррель &       лукойл \\
			topic\_160 &           сша &  американский &           штат &         обама &  вашингтон &            белый &        барак \\
			topic\_3   &         китай &     китайский &        граница &  пространство &    опасный &              кнр &        пекин \\
			topic\_43  &      миллиард &           ржд &    оцениваться &     потратить &      сумма &  железнодорожный &     железный \\
			\bottomrule
	\end{tabular}}
	\caption{Пример распределений слов в темах, полученных вероятностным тематическим моделированием, с помощью библиотеки bigARTM}
	\label{bigartm_topics}
\end{table}

Сгенерированные признаки и последующее обучение градиентного бустинга на них, значительно улучшили качество по сравнению
с предыдущим результатом ($\text{Accuracy} = 0,8687$, $\text{F1 score} = 0,8711$), однако качество линейного SVM на TF-IDF признаках
градиентный бустинг над деревьями превзойти не смог, и именно по этой причине в веб-сервисе для классификации используется SVM.
Детальное сравнение качества двух алгоритмов представлено на рисунке \ref{lgb_vs_svm}.

\section{Кластеризация новостных статей}
Пусть дана выборка $X = (x_i)_{i=1}^{\ell}, \ x_i \in \mathbb{X}$. В задаче кластеризации требуется найти в данных
$K$ кластеров --- областей, объекты внутри которых похожи друг на друга, а объекты из разных кластеров друг на друга не похожи.
Более формально, требуется построить алгоритм $a : \mathbb{X} \to \{1, \dots, K\}$, определяющий для каждого объекта номер его кластера.
Число кластеров $K$ может быть либо известно, либо являться параметром алгоритма.

В данной работе, в качестве меры семантической схожести двух новостей, используется косинусная мера.
Пусть есть две новости, которые представлены в виде двух векторов $x$ и $y$. Известно, что их скалярное произведение
и косинус угла $\alpha$ между ними связаны следующим соотношением:
$$
\langle x,y \rangle = \Vert x \Vert \Vert y \Vert \cdot \cos(\alpha).
$$
Тогда, косинусное расстояние определяется как
$$
\rho_{\cos}(x, y) = \arccos\left(\frac{\langle x,y \rangle}{\Vert x \Vert \Vert y \Vert} \right).
$$

Косинусное расстояние часто используется для измерения схожести между текстами.
Каждый документ описывается вектором, каждая компонента которого соответствует слову из словаря. В данной работе, компонента
вектора --- это TF-IDF вес слова, если оно встречается в тексте, и ноль в противном случае.
Тогда косинус между двумя векторами будет тем больше, чем больше общих слов в этих двух документах одновременно.

\subsection{Кластеризация с помощью KMeans}
В данной работе для кластеризации новостей использовалась модифицированная версия алгоритма KMeans \cite{macqueen1967}. Данный алгоритм 
оптимизирует внутрикластерное расстояние, в данной работе это был квадрат евклидовой нормы.

Модификация алгоритма, использующегося в веб-сервисе, отличается от изначального тем, что после того как были выделены центры кластеров,
новость добавляется в кластер только если косинусное расстояние между ней и центром кластера больше $0,5$.

Кроме того, кластеры упорядочиваются по среднему времени публикации новостей и среднему внутрикластерному расстоянию.

\subsection{Графовый метод кластеризации}
Данный подход кластеризации основан на алгоритме нахождения связных компонент в графе. Изначально строится полный граф,
где вершины~--- векторное представление новостей, а вес рёбер равен косинусному расстоянию между вершинами.
Рёбра, вес которых меньше определённого параметра, игнорируется.

Далее, применяется алгоритм поиска в ширину от первой вершины, не находящейся в кластере. Все вершины, по которым прошёл поиск, добавляются
в кластеры и помечаются как посещённые, затем начинается новый поиск от следующей непомеченной вершины.
Такой поиск производится до тех пор, пока все вершины не станут находится в каком-либо кластере.

Псевдокод алгоритма приведён на рисунке \ref{pseudo}.
\begin{figure}[h]
\begin{algorithm}[H]
\DontPrintSemicolon
$ comp \gets 0 $\;
\For{$uv \in E$}{
    $w(u,v) \gets \cos{(u,v)}$\;
}
\For{$v \in V$}{
    $c[v] \gets  nil $\;
}
\For{$v \in V$}{
    \If{$ c[v] = nil $} {
    	$ c[v] \gets comp $\;
		$Q \gets \{v\}$\;
		$S \gets \emptyset$\;
		\While{ $ Q \ne \emptyset$}{
		    $u \gets pop(Q)$\;
		    $c[u] \gets comp$ 
		    $S \gets S \cup \{v\}$\;
		    \For{$n \notin S, un \in E$}{
		        \If{$w(u, n) \geqslant \text{threshold}$ {\bf and} $c[n] = nil $} {
		            $Q \gets Q \cup \{n\}$\;
		        }
		    }
		}
		$ comp \gets comp + 1 $
	}
}
\end{algorithm}
\caption{Псевдокод алгоритма поиска компонент в графе}
\label{pseudo}
\end{figure}

Оценка качества кластеризации осуществлялась имперически, т.к. численные метрики, например, такие как внутрикластерное расстояние или
индекс Данна \cite{doi:10.1080/01969727308546046}, в данной задаче слабо коррелировали с тем, на сколько похожими получались новости внутри
кластеров. 

Из двух рассмотренных методов кластеризации на практике лучше себя показал графовый метод, поэтому он был включён в новостной агрегатор в качестве 
основного алгоритма кластеризации.

\section{Разработка веб-приложения}
\input{web_application_development}

\section{Заключение}

\include{bibliography} % Список литературы

\setcounter{secnumdepth}{0}
\section{Приложение}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.95\textwidth]{svm_confusion_matrix.pdf}
	\caption{Матрица ошибок линейного SVM}
	\label{svm_confusion}
\end{figure}

\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\hspace*{-2cm}
		\includegraphics[width=1\textwidth]{svm_classif_report.pdf}
		\caption{Линейный SVM}
		\label{svm_classif_report}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\hspace*{-2cm}
		\includegraphics[width=1\textwidth]{lgb_w2v_bigartm_classif_report.pdf}
		\caption{LightGBM + Word2Vec + bigARTM}
	\end{subfigure}
	\caption{Precision, Recall и F1 мера по каждому из классов}
	\label{lgb_vs_svm}
\end{figure*}

\begin{table}[h]
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{r|cccccccc}
			\toprule
			\textbf{animals}         &               жить &          вольер &             хозяин &              животный &               зоопарк &        питомец &           кличка &        животное \\
			\textbf{auto}            &          авторынок &           осаго &      автомобильный &     автопроизводитель &              автопром &          камаз &          автоваз &      автомобиль \\
			\textbf{basketball}      &                рфб &      евробаскет &          центровой &          кубок европа &             баскетбол &   баскетболист &         евролига &             нба \\
			\textbf{biathlon}        &           эстафета &    биатлонистка &            шипулин &                   сбр &            хохфильцен &        биатлон &              ibu &      биатлонист \\
			\textbf{books}           &         библиотека &    произведение &       писательница &                 роман &                 книга &   литературный &             поэт &        писатель \\
			\textbf{boxing}          &              алоян &          лебзяк &                мма &              поединок &              поветкин &            бой &             бокс &          боксер \\
			\textbf{business}        &   россельхознадзор &            fifa &                ржд &               газпром &           туроператор &        formula &         ритейлер &             оао \\
			\textbf{chess}           &            карякин &       шахматист &           карякина &                магнус &               шахматы &        карлсен &              фид &       шахматный \\
			\textbf{companies}       &  тысяча автомобиль &        компания &  миллиард кубометр &              миллиард &                тысяча &  процент акция &         ретейлер &         процент \\
			\textbf{cosmos}          &          космонавт &         светить &           прогресс &             вселенная &                космос &     астрофизик &             марс &       астронавт \\
			\textbf{crime}           &          грабитель &         изымать &        группировка &               полиция &               убивать &         летний &       преступник &          тюрьма \\
			\textbf{cybersport}      &             gaming &            team &              valve &           киберфутбол &                  dota &     киберспорт &  киберспортивный &  киберспортсмен \\
			\textbf{economics}       &               мрот &          греция &             бюджет &                пенсия &                   ввп &         минфин &        экономика &        инфляция \\
			\textbf{films}           &         мультфильм &          сериал &            актриса &                  кино &               картина &       режиссер &            актер &           фильм \\
			\textbf{football}        &               поле &         стадион &         нападающий &              матч тур &                  фифа &           уефа &        футболист &    полузащитник \\
			\textbf{forces}          &       развертывать &      выполнение &     военнослужащий &               военный &                 шойгу &        генштаб &       конашенков &      минобороны \\
			\textbf{formula1}        &              манор &      цитировать &               рено &               феррари &              макларен &          пилот &         мерседес &         формула \\
			\textbf{hockey}          &           авангард &             ска &              шайба &            нападающий &                хоккей &       хоккеист &              нхл &             кхл \\
			\textbf{internet}        &          википедия &          сервис &             ресурс &               youtube &                  сайт &          хакер &           блогер &        интернет \\
			\textbf{judiciary}       &             стража &          статья &       арестовывать &               колония &  следственный комитет &      следствие &              скр &  комитет россия \\
			\textbf{music}           &         композитор &     евровидение &              песня &                 певец &               концерт &         альбом &           певица &        музыкант \\
			\textbf{politics}        &              лидер &          кремль &      парламентарий &                партия &               депутат &        госдума &            глава &             мид \\
			\textbf{realty}          &             объект &    строительный &           жилищный &         строительство &                   жкх &        ипотека &            жилье &    недвижимость \\
			\textbf{religion}        &          монастырь &           собор &          церковный &                муфтий &                святой &       христиан &       митрополит &        патриарх \\
			\textbf{science}         &        университет &          журнал &          математик &                 физик &               научный &       археолог &    исследователь &          ученый \\
			\textbf{skiing}          &             вяльбе &          нортуг &             лыжник &                   fis &                легков &         йохауг &            лахти &         устюгов \\
			\textbf{social-networks} &          некоторые &            юзер &           facebook &               twitter &     пользователь сеть &   пользователь &        вконтакте &         соцсеть \\
			\textbf{technologies}    &          vimpelcom &           apple &           оператор &                  wifi &              говорить &            мтс &            робот &         контакт \\
			\textbf{tennis}          &    кубок федерация &            open &            шарапов &                теннис &                  корт &      теннисист &      теннисистка &     кубок дэвис \\
			\textbf{theatre}         &         росгосцирк &        цирковой &              балет &            постановка &           театральный &         мюзикл &            театр &       спектакль \\
			\textbf{volleyball}      &              факел &         маричев &             алекно &             суперлига &             казанский &      белогорье &      волейболист &        волейбол \\
			\textbf{weapons}         &               jane &  использоваться &       defense news &  министерство оборона &             миллиметр &  миллиметровый &          defense &             тип \\
			\bottomrule
	\end{tabular}}
	\caption{Слова с наибольшим весом по каждой теме (веса линейного SVM)}
	\label{table:svm_topwords}
\end{table}


\end{document}